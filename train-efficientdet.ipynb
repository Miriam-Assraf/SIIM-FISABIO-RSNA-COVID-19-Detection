{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **EfficientDet Train**","metadata":{}},{"cell_type":"markdown","source":"* [Dependencies and imports](#section-one)\n* [Basic configurations](#section-two)\n* [Overide check_box function](#sub-section-three)\n    - [check_box function](#sub-section-three-one)\n    - [_post_process function](#sub-section-three-two)\n* [Check labels distribution](#section-four)\n* [Split data to folds](#section-five)\n* [Data augmentation using Albumentations](#section-six)\n* [Custom dataset](#section-seven)\n* [Metric](#section-eight)\n* [Fitter](#section-nine)\n* [Train](#section-ten)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## **Dependencies and imports**","metadata":{}},{"cell_type":"code","source":"conda install gdcm -c conda-forge","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:05:41.603937Z","iopub.execute_input":"2021-10-26T17:05:41.604689Z","iopub.status.idle":"2021-10-26T17:06:33.793119Z","shell.execute_reply.started":"2021-10-26T17:05:41.604594Z","shell.execute_reply":"2021-10-26T17:06:33.792215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall numpy","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:06:33.794887Z","iopub.execute_input":"2021-10-26T17:06:33.795135Z","iopub.status.idle":"2021-10-26T17:06:46.623881Z","shell.execute_reply.started":"2021-10-26T17:06:33.795106Z","shell.execute_reply":"2021-10-26T17:06:46.623012Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/timm-package/timm-0.4.12-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-26T17:06:46.626010Z","iopub.execute_input":"2021-10-26T17:06:46.626289Z","iopub.status.idle":"2021-10-26T17:06:50.731336Z","shell.execute_reply.started":"2021-10-26T17:06:46.626247Z","shell.execute_reply":"2021-10-26T17:06:50.730403Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import sys\n#sys.path.append(\"../input/timm-efficientdet-pytorch\")\nsys.path.append(\"../input/efficientdet-pytorch-master/efficientdet-pytorch-master\")\nsys.path.append(\"../input/omegaconf\")\n\nimport torch\nimport os\nimport ast\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# --- time ---\nfrom datetime import datetime\nimport time\n# --- images ---\nimport cv2\nimport albumentations as A\n# --- data ---\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n# --- effdet ---\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict #DetBenchEval\nfrom effdet.efficientdet import HeadNet\n# --- wandb ---\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n# --- dicom ---\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n# --- warnings ---\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-26T17:06:50.735370Z","iopub.execute_input":"2021-10-26T17:06:50.735640Z","iopub.status.idle":"2021-10-26T17:06:58.827107Z","shell.execute_reply.started":"2021-10-26T17:06:50.735603Z","shell.execute_reply":"2021-10-26T17:06:58.826273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"offline = False\nif not offline:\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb-key\")\n    wandb.login(key=wandb_key)\n\n    run = wandb.init(project=\"siim-covid19-detection\", name=\"object_detection\", resume=True, mode='online') #resume=True,","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:06:58.828472Z","iopub.execute_input":"2021-10-26T17:06:58.828742Z","iopub.status.idle":"2021-10-26T17:07:07.158173Z","shell.execute_reply.started":"2021-10-26T17:06:58.828705Z","shell.execute_reply":"2021-10-26T17:07:07.157363Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:07.160049Z","iopub.execute_input":"2021-10-26T17:07:07.160338Z","iopub.status.idle":"2021-10-26T17:07:07.168043Z","shell.execute_reply.started":"2021-10-26T17:07:07.160299Z","shell.execute_reply":"2021-10-26T17:07:07.167327Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## **Basic configurations**","metadata":{}},{"cell_type":"code","source":"# --- configs ---\nNEGATIVE = 'negative'\nTYPICAL = 'typical'\nINDETERMINATE = 'indeterminate'\nATYPICAL = 'atypical'\n\nclass Configs:\n    img_size = 768 # 1024 896 768 640 512\n    n_folds = 6\n    thing_classes = {TYPICAL:1, INDETERMINATE:2, ATYPICAL:3}\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:07.169390Z","iopub.execute_input":"2021-10-26T17:07:07.170029Z","iopub.status.idle":"2021-10-26T17:07:07.314199Z","shell.execute_reply.started":"2021-10-26T17:07:07.169992Z","shell.execute_reply":"2021-10-26T17:07:07.313354Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sub-section-one-one\"></a>","metadata":{}},{"cell_type":"code","source":"# Read train df\ntrain_df = pd.read_csv('../input/d/miriamassraf/siim-covid19-detection/train_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:07.315720Z","iopub.execute_input":"2021-10-26T17:07:07.316038Z","iopub.status.idle":"2021-10-26T17:07:07.389369Z","shell.execute_reply.started":"2021-10-26T17:07:07.315994Z","shell.execute_reply":"2021-10-26T17:07:07.388634Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## **Override functions**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sub-section-three-one\"></a>\n### **check_box function**\nMake sure bounding boxes whithin image bounds","metadata":{}},{"cell_type":"code","source":"rows_to_change = {}\n#check if exist boxes with negative values or higher than image boundaries\nfor index, row in train_df.iterrows():\n    boxes = ast.literal_eval(row['pascal_voc_boxes'])\n    new_boxes = []\n    change = False\n    for box in boxes:\n        x1,y1,x2,y2 = box\n        img_shape = ast.literal_eval(row['image_shape'])\n        if x1<0 or y1<0 or x2>img_shape[1] or y2>img_shape[0]:\n            print(\"For image: {}\\nShape: {}\\nBounding box: {}\".format(row['img_id'], img_shape, box))\n            change = True\n        \n            if x1<0:\n                x1=0\n            if y1<0:\n                y1=0\n            if x2>img_shape[1]:\n                x2=img_shape[1]\n            if y2>img_shape[0]:\n                y2=img_shape[0]\n\n        new_boxes.append([x1,y1,x2,y2])\n        \n    if change:\n        rows_to_change[row['img_id']] = new_boxes\n        \nfor img_id, new_boxes in rows_to_change.items():\n    train_df.loc[train_df['img_id'] == img_id, 'pascal_voc_boxes'] = str(new_boxes)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:07.391018Z","iopub.execute_input":"2021-10-26T17:07:07.391497Z","iopub.status.idle":"2021-10-26T17:07:08.174882Z","shell.execute_reply.started":"2021-10-26T17:07:07.391459Z","shell.execute_reply":"2021-10-26T17:07:08.174194Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# If box bounderies exceeds image boundaries set to min/max value accordingly\ndef new_check_bbox(bbox):\n    \"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n    # added block \n    bbox=list(bbox)\n    for i in range(4):\n        if (bbox[i]<0) :\n            bbox[i]=0\n        elif (bbox[i]>1) :\n            bbox[i]=1\n    bbox=tuple(bbox)\n    # end added block \n    \n    for name, value in zip([\"x_min\", \"y_min\", \"x_max\", \"y_max\"], bbox[:4]):\n        if not 0 <= value <= 1:\n            raise ValueError(\n                \"Expected {name} for bbox {bbox} \"\n                \"to be in the range [0.0, 1.0], got {value}.\".format(bbox=bbox, name=name, value=value)\n            )\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if x_max <= x_min:\n        raise ValueError(\"x_max is less than or equal to x_min for bbox {bbox}.\".format(bbox=bbox))\n    if y_max <= y_min:\n        raise ValueError(\"y_max is less than or equal to y_min for bbox {bbox}.\".format(bbox=bbox))\n\nA.augmentations.bbox_utils.check_bbox = new_check_bbox","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.178156Z","iopub.execute_input":"2021-10-26T17:07:08.178415Z","iopub.status.idle":"2021-10-26T17:07:08.186986Z","shell.execute_reply.started":"2021-10-26T17:07:08.178381Z","shell.execute_reply":"2021-10-26T17:07:08.186313Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now we are sure there is no exceeding from image bounds for our bboxes, avoid floating point precision issues make sure bbox boundaries are in range 0,1 by adding a block to albumentations check_bbox function","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sub-section-three-two\"></a>\n### **_post_process function**\nMake sure indices are integers","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sub-section-four\"></a>\n## **Check labels distribution**","metadata":{}},{"cell_type":"code","source":"num_negatives = len(train_df[train_df['study_level'] == 'negative']) \nnum_typicals = len(train_df[train_df['study_level'] == 'typical']) \nnum_atypical = len(train_df[train_df['study_level'] == 'atypical']) \nnum_indeterminates = len(train_df[train_df['study_level'] == 'indeterminate'])\n\ncounts = {'negative': num_negatives, 'typical': num_typicals, 'atypical': num_atypical, 'indeterminate': num_indeterminates}\n\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.188261Z","iopub.execute_input":"2021-10-26T17:07:08.189042Z","iopub.status.idle":"2021-10-26T17:07:08.216218Z","shell.execute_reply.started":"2021-10-26T17:07:08.189005Z","shell.execute_reply":"2021-10-26T17:07:08.213577Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"total = counts['negative'] + counts['typical'] + counts['atypical'] + counts['indeterminate']\n\nprint('Total : ', total)\nprint('%negatives = {:.2f}'.format((counts['negative']/total) * 100))\nprint('%typicals = {:.2f}'.format((counts['typical']/total) * 100))\nprint('%atypicals = {:.2f}'.format((counts['atypical']/total) * 100))\nprint('%indeterminates = {:.2f}'.format((counts['indeterminate']/total) * 100))","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.217370Z","iopub.execute_input":"2021-10-26T17:07:08.217616Z","iopub.status.idle":"2021-10-26T17:07:08.228592Z","shell.execute_reply.started":"2021-10-26T17:07:08.217582Z","shell.execute_reply":"2021-10-26T17:07:08.227847Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We can see the data is highly imbalanced. </br>","metadata":{}},{"cell_type":"markdown","source":"### **Approaches for imbalanced data**\n1. Oversample - \"create\" new data for the less common class </br>\n2. StratifiedShuffleSplit - balanced distribution of the data to folds </br>\n3. Focal Loss","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## **Split data to folds**","metadata":{}},{"cell_type":"code","source":"class DataFolds:\n    def __init__(self, train_df, continue_train=False, debug=False):\n        assert Configs.n_folds > 0, \"num folds must be a positive number\"\n        if continue_train:\n            self.train_df = pd.read_csv('../input/d/miriamassraf/siim-covid19-detection/object_detection/splitted_train_df.csv')\n            #self.new_paths()\n        else:\n            df = train_df\n            # undersample - split frequent class and use half for each fold split\n            df1, df2 = self.undersample(df, TYPICAL, 0.5)\n            # oversample - increase size of least frequent classes for each half\n            df1 = self.oversample(df1, [INDETERMINATE, ATYPICAL], [0.4, 2.0])\n            df2 = self.oversample(df2, [INDETERMINATE, ATYPICAL], [0.4, 2.0])\n            # split each df to folds (firt 0 to Configs.n_folds, second from Configs.n_folds to 2*Configs.n_folds)\n            df1 = self.split_to_folds(df1)\n            df2 = self.split_to_folds(df2, start_from_zero=False)\n            # create a single df with all folds\n            self.train_df = df1.append(df2, ignore_index = True)\n        \n        if debug:\n            self.train_df = self.train_df.sample(frac=0.02)\n    \n    def new_paths(self):\n        paths = []\n        for index, row in self.train_df.iterrows():\n            path = row['dicom_path'].split('/')\n            del path[2]\n            path = \"/\".join(path)\n            paths.append(path)\n\n        self.train_df['dicom_path'] = paths\n\n    def oversample(self, df, classes, fracs):\n        for cls,frac in zip(classes, fracs):\n            rows_to_add = df[df['study_level']==cls].sample(frac=frac, replace=True)\n            df = df.append(rows_to_add, ignore_index = True)\n        return df\n    \n    def undersample(self, df, cls, frac):\n        freq_cls = df[df['study_level']==cls]\n        half1 = freq_cls.sample(frac=frac, replace=True)\n        half2 = freq_cls[~freq_cls['img_id'].isin(half1['img_id'])]\n        df1 = df[df['study_level']!=cls].append(half1, ignore_index = True)\n        df2 = df[df['study_level']!=cls].append(half2, ignore_index = True)\n        return df1, df2\n            \n    def split_to_folds(self, df, start_from_zero=True):\n        skf = StratifiedKFold(n_splits=Configs.n_folds//2)\n        for n, (train_index, val_index) in enumerate(skf.split(X=df.index, y=df['int_label'])):\n            if start_from_zero:\n                df.loc[df.iloc[val_index].index, 'fold'] = int(n)\n            else:\n                df.loc[df.iloc[val_index].index, 'fold'] = int(n+(Configs.n_folds//2))\n        return df\n    \n    def get_train_df(self, fold_number): \n        if fold_number >= 0 and fold_number < Configs.n_folds:\n            return self.train_df[self.train_df['fold'] != fold_number]\n\n    def get_val_df(self, fold_number):\n        if fold_number >= 0 and fold_number < Configs.n_folds:\n            return self.train_df[self.train_df['fold'] == fold_number]","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.229932Z","iopub.execute_input":"2021-10-26T17:07:08.230622Z","iopub.status.idle":"2021-10-26T17:07:08.247978Z","shell.execute_reply.started":"2021-10-26T17:07:08.230587Z","shell.execute_reply":"2021-10-26T17:07:08.247228Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_folds = DataFolds(train_df, continue_train=True)\n#data_folds.train_df.to_csv(\"splitted_train_df.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.249021Z","iopub.execute_input":"2021-10-26T17:07:08.250674Z","iopub.status.idle":"2021-10-26T17:07:08.348016Z","shell.execute_reply.started":"2021-10-26T17:07:08.250636Z","shell.execute_reply":"2021-10-26T17:07:08.347278Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Visualize distribution of labels over folds**","metadata":{}},{"cell_type":"code","source":"# Plot distibution\ndef plot_folds(data_folds):\n    nrows = Configs.n_folds//2\n    if Configs.n_folds%2 != 0:\n        nrows += 1\n    \n    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize=(30,15))\n    row = 0\n    for fold in range(Configs.n_folds):\n        if fold%2 == 0:\n            col = 0\n            if fold != 0:\n                row += 1\n        else:\n            col = 1\n\n        labels_count = {}\n        labels_count[TYPICAL] = len(data_folds.train_df[((data_folds.train_df['fold'] == fold)&(data_folds.train_df['study_level'] == TYPICAL))])\n        labels_count[ATYPICAL] = len(data_folds.train_df[((data_folds.train_df['fold'] == fold)&(data_folds.train_df['study_level'] == ATYPICAL))])\n        labels_count[INDETERMINATE] = len(data_folds.train_df[((data_folds.train_df['fold'] == fold)&(data_folds.train_df['study_level'] == INDETERMINATE))])\n\n        ax[row, col].bar(list(labels_count.keys()), list(labels_count.values()))\n\n        for j, value in enumerate(labels_count.values()):\n            ax[row, col].text(j, value+2, str(value), color='#267DBE', fontweight='bold')\n\n        ax[row, col].grid(axis='y', alpha=0.75)\n        ax[row, col].set_title(\"For fold #{}\".format(fold), fontsize=15)\n        ax[row, col].set_ylabel(\"count\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-26T17:07:08.349380Z","iopub.execute_input":"2021-10-26T17:07:08.349641Z","iopub.status.idle":"2021-10-26T17:07:08.360792Z","shell.execute_reply.started":"2021-10-26T17:07:08.349605Z","shell.execute_reply":"2021-10-26T17:07:08.360010Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plot_folds(data_folds)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:08.362202Z","iopub.execute_input":"2021-10-26T17:07:08.362676Z","iopub.status.idle":"2021-10-26T17:07:09.195539Z","shell.execute_reply.started":"2021-10-26T17:07:08.362637Z","shell.execute_reply":"2021-10-26T17:07:09.194850Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## **Data augmentation using Albumentations**","metadata":{}},{"cell_type":"code","source":"def get_transforms(train: bool=True):\n    if train:\n        return A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.Rotate(limit=10),\n            A.OneOf([\n                A.HueSaturationValue(), \n                A.RandomBrightnessContrast(),\n                A.CLAHE(p=0.6),\n            ], p=0.4),\n            A.OneOf([\n                A.Blur(blur_limit=3, p=0.5),\n                A.MedianBlur(blur_limit=3, p=0.5),\n                A.GaussNoise(p=0.5),\n                A.Sharpen(p=0.5)\n                ],p=0.4),\n            A.Resize(height=Configs.img_size, width=Configs.img_size, p=1),], \n            bbox_params=A.BboxParams(format='pascal_voc',\n                                     min_area=0, \n                                     min_visibility=0,\n                                     label_fields=['labels'])\n        )\n\n    else:\n        # for validation only resize image\n        return A.Compose([\n            A.Resize(height=Configs.img_size, width=Configs.img_size, p=1),], \n            bbox_params=A.BboxParams(format='pascal_voc',\n                                     min_area=0, \n                                     min_visibility=0,\n                                     label_fields=['labels'])\n        )","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.197100Z","iopub.execute_input":"2021-10-26T17:07:09.197599Z","iopub.status.idle":"2021-10-26T17:07:09.207663Z","shell.execute_reply.started":"2021-10-26T17:07:09.197561Z","shell.execute_reply":"2021-10-26T17:07:09.206883Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## **Custom dataset**","metadata":{}},{"cell_type":"code","source":"def get_dicom_img(path):\n    data_file = pydicom.dcmread(path)\n    img = apply_voi_lut(data_file.pixel_array, data_file)\n\n    if data_file.PhotometricInterpretation == \"MONOCHROME1\":\n        img = np.amax(img) - img\n    \n    # Rescaling grey scale between 0-255 and convert to uint\n    img = img - np.min(img)\n    img = img / np.max(img)\n    img = (img * 255).astype(np.uint8)\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.209175Z","iopub.execute_input":"2021-10-26T17:07:09.209509Z","iopub.status.idle":"2021-10-26T17:07:09.220701Z","shell.execute_reply.started":"2021-10-26T17:07:09.209474Z","shell.execute_reply":"2021-10-26T17:07:09.220001Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Covid19Dataset(Dataset):\n    def __init__(self, df, train=True, transform=None):\n        super().__init__()\n        self.df = df\n        self.train = train\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = row['dicom_path']\n        \n        img = get_dicom_img(img_path)\n        bboxes = ast.literal_eval(row['pascal_voc_boxes'])\n        if row['num_boxes'] > 0:\n            labels = [row['int_label']]*row['num_boxes']\n        else:\n            labels = [row['int_label']]\n            \n        if self.transform:\n            transformed = self.transform(**{'image': img, 'bboxes': bboxes, 'labels': labels})\n            img = transformed['image']\n            transformed_bboxes = transformed['bboxes']        \n            \n            if row['num_boxes'] > 0:\n                bboxes = transformed_bboxes\n\n        # normalize img\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img /= 255.0\n        \n        # convert everything into a torch.Tensor\n        img = torch.as_tensor(img, dtype=torch.float32)\n        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        idx = torch.tensor([idx])\n        \n        #bboxes = torch.stack(tuple(map(torch.tensor, zip(*bboxes)))).permute(1, 0)\n        bboxes[:,[0,1,2,3]] = bboxes[:,[1,0,3,2]]  #yxyx: be warning\n        \n        # targets for object detection\n        target = {}\n        target['bbox'] = bboxes\n        target['cls'] = labels\n        target['img_id'] = idx\n                    \n        # permute image to [C,H,W] from [H,W,C] and normalize\n        img = img.permute(2, 0, 1)\n        \n        return img, target, row['img_id']\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.221835Z","iopub.execute_input":"2021-10-26T17:07:09.222172Z","iopub.status.idle":"2021-10-26T17:07:09.237552Z","shell.execute_reply.started":"2021-10-26T17:07:09.222136Z","shell.execute_reply":"2021-10-26T17:07:09.236846Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# helper function to get train/validation data by fold\ndef get_dataset_fold(data_folds, fold,train=True):\n    if train:\n        return Covid19Dataset(data_folds.get_train_df(fold), train=True, transform=get_transforms(train))\n    return Covid19Dataset(data_folds.get_val_df(fold), train=False, transform=get_transforms(train))","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.238903Z","iopub.execute_input":"2021-10-26T17:07:09.239319Z","iopub.status.idle":"2021-10-26T17:07:09.248310Z","shell.execute_reply.started":"2021-10-26T17:07:09.239284Z","shell.execute_reply":"2021-10-26T17:07:09.247451Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Visualize dataset**","metadata":{}},{"cell_type":"code","source":"train_dataset = get_dataset_fold(data_folds, 0)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.249896Z","iopub.execute_input":"2021-10-26T17:07:09.250166Z","iopub.status.idle":"2021-10-26T17:07:09.265883Z","shell.execute_reply.started":"2021-10-26T17:07:09.250132Z","shell.execute_reply":"2021-10-26T17:07:09.264997Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"image, target, image_id = train_dataset[3]\nimage = image.numpy()\nimage = np.transpose(image, (1,2,0))\n\nprint(target)\nboxes = target['bbox']\nnew_img = np.copy(image)\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    y1,x1,y2,x2 = box\n    cv2.rectangle(new_img,\n                  (int(x1), int(y1)), (int(x2),  int(y2)), (0, 255, 0), Configs.img_size//200)\n    \nax.set_axis_off()\nax.imshow(new_img)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:09.267819Z","iopub.execute_input":"2021-10-26T17:07:09.268169Z","iopub.status.idle":"2021-10-26T17:07:12.168147Z","shell.execute_reply.started":"2021-10-26T17:07:09.268060Z","shell.execute_reply":"2021-10-26T17:07:12.167469Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## **Metric**","metadata":{}},{"cell_type":"markdown","source":"The competitions metric is PASCAL VOC 2010 mean average precision (mAP) at IoU > 0.5","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/chenyc15/mean-average-precision-metric \ndef calculate_iou(gt, pred):\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pred: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    \n    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n    pred_x1, pred_y1, pred_x2, pred_y2 = pred\n\n    # Calculate overlap area\n    dx = min(gt_x2, pred_x2) - max(gt_x1, pred_x1) + 1\n    \n    if dx < 0:\n        return 0.0\n    \n    dy = min(gt_y2, pred_y2) - max(gt_y1, pred_y1) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt_x2 - gt_x1 + 1) * (gt_y2 - gt_x1 + 1) +\n            (pred_x2 - pred_x1 + 1) * (pred_y2 - pred_y1 + 1) -\n            overlap_area\n    )\n\n    return overlap_area / union_area\n\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, ious=None):\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n\n    for gt_idx in range(len(gts)):\n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box (set to -1)\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\ndef calculate_image_precision_recall(gts, preds, threshold = 0.5):\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    ious = np.ones((len(gts), len(preds))) * -1\n    \n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    # for pred_idx, pred in enumerate(preds_sorted):\n    for pred_idx in range(n):\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n    \n    precision = tp / (tp + fp)\n    recall = tp / len(gts)\n    \n    return precision, recall\n\n# https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/evaluation/metrics.py\ndef compute_average_precision(precision: np.ndarray, recall: np.ndarray):\n    \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n    Precision is modified to ensure that it does not decrease as recall\n    decrease.\n    Args:\n        precision: A float [N, 1] numpy array of precisions\n        recall: A float [N, 1] numpy array of recalls\n    Raises:\n        ValueError: if the input is not of the correct format\n    Returns:\n        average_precison: The area under the precision recall curve. NaN if\n            precision and recall are None.\n    \"\"\"\n    if precision is None:\n        if recall is not None:\n            raise ValueError(\"If precision is None, recall must also be None\")\n        return np.NAN\n\n    if len(precision) != len(recall):\n        raise ValueError(\"precision and recall must be of the same size.\")\n\n    if np.amin(precision) < 0 or np.amax(precision) > 1:\n        raise ValueError(\"Precision must be in the range of [0, 1].\")\n    if np.amin(recall) < 0 or np.amax(recall) > 1:\n        raise ValueError(\"recall must be in the range of [0, 1].\")\n    if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n        raise ValueError(\"recall must be a non-decreasing array\")\n    \n    # recall sorted in increasing order with values 0-1\n    recall = np.concatenate([[0], recall, [1]])\n    # precision as well, but we need max precision so we don't concatenate with 1 at the end\n    precision = np.concatenate([[0], precision, [0]])  \n\n    # \"smooth\" curves to rectangles by getting the max value\n    for i in range(len(precision) - 2, -1, -1):\n        precision[i] = np.maximum(precision[i], precision[i + 1])\n    \n    # calculate the sum of rectangle areas\n    indices = np.where(recall[1:] != recall[:-1])[0] + 1\n    average_precision = np.sum((recall[indices] - recall[indices - 1]) * precision[indices])\n    return average_precision","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.169372Z","iopub.execute_input":"2021-10-26T17:07:12.169608Z","iopub.status.idle":"2021-10-26T17:07:12.192969Z","shell.execute_reply.started":"2021-10-26T17:07:12.169575Z","shell.execute_reply":"2021-10-26T17:07:12.192095Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-nine\"></a>\n## **Fitter**","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.194128Z","iopub.execute_input":"2021-10-26T17:07:12.194512Z","iopub.status.idle":"2021-10-26T17:07:12.206451Z","shell.execute_reply.started":"2021-10-26T17:07:12.194476Z","shell.execute_reply":"2021-10-26T17:07:12.205753Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Fitter:\n    def __init__(self, dir):\n        self.epoch = 0\n        # create effdet model\n        self.model = get_net()\n        self.device = Configs.device\n        \n        # dir to save outputs\n        self.dir = dir\n        if not os.path.exists(self.dir):\n            os.makedirs(self.dir)\n        \n        self.log_path = os.path.join(self.dir, 'log.txt')\n        self.best_summary_loss = 10**5\n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=TrainGlobalConfig.lr)\n        self.scheduler = TrainGlobalConfig.SchedulerClass(self.optimizer, **TrainGlobalConfig.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n        \n    def fit(self, fold, train_loader, validation_loader, continue_train=False):\n        if continue_train:\n            path = f'../input/d/miriamassraf/siim-covid19-detection/object_detection/effdet_d7_fold{fold}/last-checkpoint.bin'\n            self.load(path)\n        else:\n            self.log(f\"Fold {fold}\")\n            \n        while self.epoch < TrainGlobalConfig.n_epochs:\n            if TrainGlobalConfig.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n\n            # train one epoch\n            t = time.time()\n            summary_loss, summary_box_loss, summary_class_loss = self.train_one_epoch(train_loader)\n            # log train losses to console/log file\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch},\\t' + \\\n                     f'total loss: {summary_loss.avg:.5f},\\t' + \\\n                     f'loss_cls: {summary_class_loss.avg:.5f},\\t' + \\\n                     f'loss_box_reg: {summary_box_loss.avg:.5f},\\t' + \\\n                     f'time: {(time.time() - t):.5f}')\n            # log train losses to wandb\n            if not offline:\n                run.log({f\"train/total_loss_fold{fold}\": summary_loss.avg})\n                run.log({f\"train/loss_box_reg_fold{fold}\": summary_box_loss.avg})\n                run.log({f\"train/loss_cls_fold{fold}\": summary_class_loss.avg})\n            \n            # save last checkpoint\n            self.save(f'{self.dir}/last-checkpoint.bin')\n            \n            # validate one epoch\n            t = time.time()\n            summary_loss, summary_box_loss, summary_class_loss, mAP = self.validation_one_epoch(validation_loader)\n            \n            # log val losses to console/log file\n            if mAP is not None:\n                self.log(f'[RESULT]: Val. Epoch: {self.epoch},\\ttotal loss: {summary_loss.avg:.5f},\\t' + \\\n                     f'loss_cls: {summary_class_loss.avg:.5f},\\t' + \\\n                     f'loss_box_reg: {summary_box_loss.avg:.5f},\\t' + \\\n                     f'time: {(time.time() - t):.5f}\\n' + \\\n                     '-'*100 + \\\n                     f'\\nmAP@IoU=0.5: {mAP},\\n' + \\\n                     '-'*100)\n            else:\n                 self.log(f'[RESULT]: Val. Epoch: {self.epoch},\\ttotal loss: {summary_loss.avg:.5f},\\t' + \\\n                     f'loss_cls: {summary_class_loss.avg:.5f},\\t' + \\\n                     f'loss_box_reg: {summary_box_loss.avg:.5f},\\t' + \\\n                     f'time: {(time.time() - t):.5f}')   \n\n            # log val losses to wandb\n            if not offline:\n                run.log({f\"val/total_loss_fold{fold}\": summary_loss.avg})\n                run.log({f\"val/loss_box_reg_fold{fold}\": summary_box_loss.avg})\n                run.log({f\"val/loss_cls_fold{fold}\": summary_class_loss.avg})\n                if (self.epoch+1)%10 == 0 and self.epoch != 0:\n                    run.log({f\"mAP_fold{fold}/IoU=0.5\": mAP})\n                \n            # update best val losses and save best checkpoint if needed\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(os.path.join(self.dir, 'best-checkpoint.bin'))\n                if not offline:\n                    wandb.save(os.path.join(self.dir, 'best-checkpoint.bin'))\n                for path in sorted(glob(os.path.join(self.dir, 'best-checkpoint.bin')))[:-3]:\n                    os.remove(path)\n            \n            # perform scheduler step\n            if TrainGlobalConfig.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n    \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        summary_box_loss = AverageMeter()\n        summary_class_loss = AverageMeter()\n        \n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if TrainGlobalConfig.verbose:\n                print(f'Train Step {step}/{len(train_loader)},\\t' + \\\n                    f'total_loss: {summary_loss.avg:.5f},\\t' + \\\n                    f'loss_cls: {summary_class_loss.avg:.5f},\\t' + \\\n                    f'loss_box_reg: {summary_box_loss.avg:.5f},\\t' + \\\n                    f'time: {(time.time() - t):.5f}', end='\\r'\n                )\n                    \n            images = torch.stack(images)\n            batch_size = images.shape[0]\n            \n            images = images.to(self.device).float()\n            boxes = [target['bbox'].to(self.device).float() for target in targets]\n            classes = [target['cls'].to(self.device).float() for target in targets]\n            targets = {'bbox':boxes, 'cls':classes}\n           \n\n            self.optimizer.zero_grad()\n            \n            # output = {'loss': loss, 'class_loss': class_loss, 'box_loss': box_loss}\n            #loss, class_loss, box_loss = self.model(images, boxes, labels)\n            output  = self.model(images, targets)\n            loss, class_loss, box_loss = output['loss'], output['class_loss'], output['box_loss']\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n            summary_box_loss.update(box_loss.detach().item(), batch_size)\n            summary_class_loss.update(class_loss.detach().item(), batch_size)\n            \n            self.optimizer.step()\n            del images, targets, image_ids\n            \n        return summary_loss, summary_box_loss, summary_class_loss\n    \n    def validation_one_epoch(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        summary_box_loss = AverageMeter()\n        summary_class_loss = AverageMeter()\n        precisions = []\n        recalls = []\n        \n        t = time.time()\n            \n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if TrainGlobalConfig.verbose:\n                print(\n                    f'Val Step {step}/{len(val_loader)}, ' + \\\n                    f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                    f'time: {(time.time() - t):.5f}', end='\\r'\n                )\n            with torch.no_grad():\n                # predict and get losses for validation batch\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                \n                images = images.to(self.device).float()\n                boxes = [target['bbox'].to(self.device).float() for target in targets]\n                classes = [target['cls'].to(self.device).float() for target in targets]\n                #scales = torch.tensor([1.])\n                #sizes = torch.tensor([Configs.img_size])\n                scales = torch.tensor([1. for target in targets]).to(self.device).float()\n                sizes = torch.tensor([(Configs.img_size, Configs.img_size) for target in targets]).to(self.device).float()\n            \n                targets = {'bbox':boxes, 'cls':classes, 'img_scale':scales, 'img_size':sizes}\n\n                output  = self.model(images, targets)\n                loss, class_loss, box_loss = output['loss'], output['class_loss'], output['box_loss']\n                \n                summary_loss.update(loss.detach().item(), batch_size)\n                summary_box_loss.update(box_loss.detach().item(), batch_size)\n                summary_class_loss.update(class_loss.detach().item(), batch_size)\n                \n                # evaluate mAP every 5 epochs (4, 9, 14, 19, 24, 29)\n                if (self.epoch+1)%5 == 0: \n                    # get prediction\n                    eval_model = get_net(train=False)\n                    eval_model.eval();\n                    state_dict = self.model.state_dict()\n                    #state_dict['anchors.boxes'] = state_dict.pop('anchor_labeler.anchors.boxes')\n                    eval_model.load_state_dict(state_dict)\n\n                    preds = eval_model(images) \n                    for i, gt_boxes in enumerate(boxes):               \n                        precision, recall = self.calc_precision_recall(preds, gt_boxes, i)\n                        precisions.append(precision)\n                        recalls.append(recall)\n                    \n            del images, targets, image_ids\n            \n        mAP = None\n        if (self.epoch+1)%5 == 0:\n            mAP = self.calc_mAP(precisions, recalls)\n        \n        return summary_loss, summary_box_loss, summary_class_loss, mAP\n    \n    def calc_precision_recall(self, preds, gt_boxes, i):\n        pred_boxes = preds[i].detach().cpu().numpy()[:,:4]\n        pred_scores = preds[i].detach().cpu().numpy()[:,4]\n        pred_labels = preds[i].detach().cpu().numpy()[:, 5]\n\n        # sort predictions by score\n        preds_sorted_idx = np.argsort(pred_scores)[::-1]\n        preds_sorted_boxes = pred_boxes[preds_sorted_idx]\n\n        return calculate_image_precision_recall(gt_boxes.detach().cpu().numpy(), preds_sorted_boxes)\n                        \n    def calc_mAP(self, precisions, recalls):\n        # sort by recall (increasing order)\n        recalls = np.array(recalls)\n        precisions = np.array(precisions)\n        sorted_idx = np.argsort(recalls)\n        recalls = recalls[sorted_idx]\n        precisions = precisions[sorted_idx]\n        return compute_average_precision(precisions, recalls)\n            \n    def to_xyxy_format(self, boxes):   \n        new_boxes = []\n        for box in boxes:\n            x,y,w,h = box\n            x1=x\n            x2=x+w\n            y1=y\n            y2=y+h\n            new_boxes.append(np.array([x1,y1,x2,y2]))\n\n        return np.array(new_boxes)  \n    \n    # save checkpoint to given path\n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    # load checkpoint from given path\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n    \n    # log to console and log file\n    def log(self, message):\n        if TrainGlobalConfig.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.207913Z","iopub.execute_input":"2021-10-26T17:07:12.208326Z","iopub.status.idle":"2021-10-26T17:07:12.267851Z","shell.execute_reply.started":"2021-10-26T17:07:12.208266Z","shell.execute_reply":"2021-10-26T17:07:12.266932Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Create effdet model**","metadata":{}},{"cell_type":"code","source":"def get_net(architecture='tf_efficientdet_d7', train=True):\n    config = get_efficientdet_config(architecture)\n    config.num_classes = len(Configs.thing_classes)\n    config.image_size = (Configs.img_size,Configs.img_size)\n    config.gamma = 2\n    net = EfficientDet(config, pretrained_backbone=True)\n    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n    \n    if train:\n        print(config)\n        return DetBenchTrain(net, config).to(Configs.device)\n\n    else:\n        #model = DetBenchEval(net, config)\n        #model.eval();\n        #return model\n        return DetBenchPredict(net).to(Configs.device)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.269337Z","iopub.execute_input":"2021-10-26T17:07:12.269785Z","iopub.status.idle":"2021-10-26T17:07:12.283518Z","shell.execute_reply.started":"2021-10-26T17:07:12.269696Z","shell.execute_reply":"2021-10-26T17:07:12.282801Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-ten\"></a>\n## **Train**","metadata":{}},{"cell_type":"markdown","source":"**Train configurations**","metadata":{}},{"cell_type":"code","source":"class TrainGlobalConfig:\n    n_epochs = 30\n    num_workers = 8\n    batch_size = 2\n    lr = 0.001\n    verbose = True\n    validation_scheduler = True  \n\n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.1,\n        patience=2,\n        verbose=True, \n        threshold=0.0001,\n        threshold_mode='abs',\n        min_lr=1e-8,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.284607Z","iopub.execute_input":"2021-10-26T17:07:12.285239Z","iopub.status.idle":"2021-10-26T17:07:12.294982Z","shell.execute_reply.started":"2021-10-26T17:07:12.285203Z","shell.execute_reply":"2021-10-26T17:07:12.294263Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Run train** ","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training(fold, train_dataset, val_dataset, continue_train=False):\n    # create tain/validation data loaders\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(val_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n    \n    # create fitter for model\n    fitter = Fitter(f'./effdet_d7_fold{fold}')\n    # run train by calling fit function\n    fitter.fit(fold, train_loader, val_loader, continue_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.299188Z","iopub.execute_input":"2021-10-26T17:07:12.299393Z","iopub.status.idle":"2021-10-26T17:07:12.309709Z","shell.execute_reply.started":"2021-10-26T17:07:12.299370Z","shell.execute_reply":"2021-10-26T17:07:12.308991Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**Run train for 5 models over the different folds**","metadata":{}},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.311103Z","iopub.execute_input":"2021-10-26T17:07:12.311434Z","iopub.status.idle":"2021-10-26T17:07:12.322541Z","shell.execute_reply.started":"2021-10-26T17:07:12.311397Z","shell.execute_reply":"2021-10-26T17:07:12.321761Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fold = 4\ntrain_dataset = get_dataset_fold(data_folds, fold)\nval_dataset = get_dataset_fold(data_folds, fold, train=False)\n\nrun_training(fold, train_dataset, val_dataset, continue_train=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T17:07:12.324135Z","iopub.execute_input":"2021-10-26T17:07:12.327281Z","iopub.status.idle":"2021-10-26T21:30:48.382776Z","shell.execute_reply.started":"2021-10-26T17:07:12.327253Z","shell.execute_reply":"2021-10-26T21:30:48.379636Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!zip -r ./effdet_d7_fold4.zip ./effdet_d7_fold4","metadata":{"execution":{"iopub.status.busy":"2021-10-26T21:30:51.809481Z","iopub.execute_input":"2021-10-26T21:30:51.812427Z","iopub.status.idle":"2021-10-26T21:31:52.364392Z","shell.execute_reply.started":"2021-10-26T21:30:51.812367Z","shell.execute_reply":"2021-10-26T21:31:52.363482Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('./effdet_d7_fold4.zip')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T21:31:52.368440Z","iopub.execute_input":"2021-10-26T21:31:52.368683Z","iopub.status.idle":"2021-10-26T21:31:52.376953Z","shell.execute_reply.started":"2021-10-26T21:31:52.368652Z","shell.execute_reply":"2021-10-26T21:31:52.376239Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for fold in range(Configs.n_folds):\n    train_dataset = get_dataset_fold(data_folds, fold)\n    val_dataset = get_dataset_fold(data_folds, fold, train=False)\n\n    run_training(fold, train_dataset, val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T08:47:12.719737Z","iopub.status.idle":"2021-10-22T08:47:12.72031Z","shell.execute_reply.started":"2021-10-22T08:47:12.720061Z","shell.execute_reply":"2021-10-22T08:47:12.720087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**zip results and save files**","metadata":{}},{"cell_type":"code","source":"!zip -r ./effdet_d7_fold0.zip ./effdet_d7_fold0\n!zip -r ./effdet_d7_fold1.zip ./effdet_d7_fold1\n!zip -r ./effdet_d7_fold2.zip ./effdet_d7_fold2\n!zip -r ./effdet_d7_fold3.zip ./effdet_d7_fold3\n!zip -r ./effdet_d7_fold4.zip ./effdet_d7_fold4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('./effdet_d7_fold0.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('./effdet_d7_fold1.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('./effdet_d7_fold2.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('./effdet_d7_fold3.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('./effdet_d7_fold4.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}